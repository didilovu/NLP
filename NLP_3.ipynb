{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1e749ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "from gensim.models import FastText as ft\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ccb8e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VALID_BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "EMBED_DIM = 300\n",
    "\n",
    "MODEL_PATH = \"state_dict.pt\"\n",
    "TRAINING_FILE = \"ner_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e806bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1b5de44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3247/3278747014.py:1: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  fasttext_model = ft.load_fasttext_format(\"cc.en.300.bin\")\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = ft.load_fasttext_format(\"cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ccf62ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDataset:\n",
    "    def __init__(self, texts, tags):\n",
    "        self.texts = texts\n",
    "        self.tags = tags\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        tags = np.array(self.tags[item])\n",
    "    \n",
    "        ids = [fasttext_model.wv[s] for s in text]\n",
    "        \n",
    "        ids_pad = np.zeros((MAX_LEN, EMBED_DIM))\n",
    "        tags_pad = np.zeros(MAX_LEN)\n",
    "        mask = np.zeros(MAX_LEN)\n",
    "\n",
    "        for i, emb in enumerate(ids):\n",
    "            if i >= MAX_LEN:\n",
    "                break\n",
    "            ids_pad[i] = emb\n",
    "            tags_pad[i] = tags[i]\n",
    "            mask[i] = 1\n",
    "        \n",
    "        return (torch.tensor(ids_pad, dtype=torch.float32),\n",
    "                torch.tensor(tags_pad, dtype=torch.long),\n",
    "                torch.tensor(mask, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "611905e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(output, target, mask, num_labels):\n",
    "    lfn = nn.CrossEntropyLoss()\n",
    "    active_loss = mask.view(-1) == 1\n",
    "    active_logits = output.view(-1, num_labels)\n",
    "    active_labels = torch.where(\n",
    "        active_loss,\n",
    "        target.view(-1),\n",
    "        torch.tensor(lfn.ignore_index).type_as(target)\n",
    "    )\n",
    "    loss = lfn(active_logits, active_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "51541313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_stat(pred, target, mask):\n",
    "    mask = mask.bool()\n",
    "    pred = torch.masked_select(pred, mask)\n",
    "    target = torch.masked_select(target, mask)\n",
    "    \n",
    "    correct = (pred == target).sum().item()# сколько элементов угадано корректно\n",
    "    total = mask.sum().item()# сколько элементов было всего, не считая \"пустых\" с нулями\n",
    "    \n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c2c43401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_stat(torch.tensor([1,2,3,4,0,0,0,0]), torch.tensor([1,2,3,4,5,5,5,5]), torch.tensor([1,1,1,1,0,0,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9b52c240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_stat(torch.tensor([1,2,3,4,0,0,0,0]), torch.tensor([1,2,3,4,5,5,5,5]), torch.tensor([0,0,0,0,1,1,1,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e503bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityModel(nn.Module):\n",
    "    def __init__(self, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob,\n",
    "                            batch_first=True, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, embeds, hidden):\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        num_directions = 2 if self.lstm.bidirectional else 1\n",
    "        print(\"num_directions\", num_directions)\n",
    "        h_zeros = torch.zeros(self.n_layers * num_directions,\n",
    "                              batch_size, self.hidden_dim,\n",
    "                              dtype=torch.float32, device=device)\n",
    "        c_zeros = torch.zeros(self.n_layers * num_directions,\n",
    "                              batch_size, self.hidden_dim,\n",
    "                              dtype=torch.float32, device=device)\n",
    "\n",
    "        return (h_zeros, c_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a58e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_path):\n",
    "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
    "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
    "\n",
    "    enc_tag = preprocessing.LabelEncoder()\n",
    "\n",
    "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
    "\n",
    "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
    "    return sentences, tag, enc_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "136b9b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "sentences, tag, enc_tag = process_data(TRAINING_FILE)\n",
    "meta_data = {\n",
    "    \"enc_tag\": enc_tag\n",
    "}\n",
    "\n",
    "joblib.dump(meta_data, \"punch_meta.bin\")\n",
    "\n",
    "num_tag = len(list(enc_tag.classes_))\n",
    "\n",
    "train_sentences,\n",
    "test_sentences,\n",
    "train_tag,\n",
    "test_tag = train_test_split(sentences, tag, test_size=0.2, random_state = 42)# делим на трейн и тест с помощью train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "45e6996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EntityDataset(\n",
    "    texts=train_sentences, tags=train_tag\n",
    ")\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=1,\n",
    "    shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "valid_dataset = EntityDataset(\n",
    "    texts=test_sentences, tags=test_tag\n",
    ")\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1,\n",
    "    shuffle=False, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "27181cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, valid_data_loader):\n",
    "    h = model.init_hidden(VALID_BATCH_SIZE)\n",
    "    losses = []\n",
    "    \n",
    "    correct_sum, total_sum = 0, 0\n",
    "    \n",
    "    for inputs, labels, mask in valid_data_loader:\n",
    "        h = tuple([each.data for each in h])\n",
    "        inputs, labels, mask = inputs.to(device), labels.to(device), mask.to(device)  # Отправим inputs, labels и mask на GPU\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = loss_fn(output, labels.flatten(), mask, num_tag)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        correct, total = acc_stat(torch.argmax(output, dim=-1).flatten(), labels.flatten(), mask.flatten())\n",
    "        correct_sum += correct\n",
    "        total_sum += total\n",
    "    return losses, correct_sum / total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3ad6200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "model = EntityModel(num_tag, EMBED_DIM, hidden_dim, n_layers, drop_prob=0.5, bidirectional=False)\n",
    "model.to(device)\n",
    "\n",
    "lr=0.005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "097d0a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_directions 1\n",
      "num_directions 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal TypeError: float() argument must be a string or a number, not 'list'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_3247/3789666946.py\", line 23, in __getitem__\n    tags_pad[i] = tags[i]\nValueError: setting an array element with a sequence.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m%\u001b[39m print_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     33\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 34\u001b[0m     val_losses, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_data_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     37\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(val_losses)\n",
      "Cell \u001b[0;32mIn[74], line 7\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(model, valid_data_loader)\u001b[0m\n\u001b[1;32m      3\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m correct_sum, total_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels, mask \u001b[38;5;129;01min\u001b[39;00m valid_data_loader:\n\u001b[1;32m      8\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([each\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mfor\u001b[39;00m each \u001b[38;5;129;01min\u001b[39;00m h])\n\u001b[1;32m      9\u001b[0m     inputs, labels, mask \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device), mask\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Отправим inputs, labels и mask на GPU\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1224\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1250\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1250\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_utils.py:457\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal TypeError: float() argument must be a string or a number, not 'list'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_3247/3789666946.py\", line 23, in __getitem__\n    tags_pad[i] = tags[i]\nValueError: setting an array element with a sequence.\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "print_every = 10\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "writer = SummaryWriter('logs')\n",
    "\n",
    "\n",
    "model.train()\n",
    "for i in range(EPOCHS):\n",
    "    h = model.init_hidden(TRAIN_BATCH_SIZE)\n",
    "    \n",
    "    correct_sum, total_sum = 0, 0\n",
    "    \n",
    "    for inputs, labels, mask in train_data_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        mask = mask.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = loss_fn(output, labels.flatten(), mask, num_tag)\n",
    "        loss.backward()\n",
    "        correct, total = acc_stat(torch.argmax(output, dim=-1).flatten(), labels.flatten(), mask.flatten())# вызываем функцию acc_stat\n",
    "        correct_sum += correct\n",
    "        total_sum += total\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()# градиентный спуск\n",
    "        \n",
    "        if counter % print_every == 0:\n",
    "            model.eval()\n",
    "            val_losses, val_acc = eval_model(model, valid_data_loader)\n",
    "            model.train()\n",
    "            \n",
    "            val_loss = np.mean(val_losses)\n",
    "            writer.add_scalar('train/loss', loss.item(), counter)\n",
    "            writer.add_scalar('val/loss', val_loss, counter)\n",
    "            writer.add_scalar('train/acc', correct_sum / total_sum, counter)\n",
    "            writer.add_scalar('val/acc', val_acc, counter)\n",
    "\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, EPOCHS),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(val_loss),\n",
    "                  \"Train Acc: {:.6f}\".format(correct_sum / total_sum),\n",
    "                  \"Val Acc: {:.6f}\".format(val_acc))\n",
    "                \n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), MODEL_PATH)\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3c40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = joblib.load(\"meta.bin\")\n",
    "enc_tag = meta_data[\"enc_tag\"]\n",
    "\n",
    "num_tag = len(list(enc_tag.classes_))\n",
    "\n",
    "text = \"Natasha went to New York and was never seen after that...\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "torch.no_grad():\n",
    "    inputs = torch.tensor([fasttext_model.wv[s] for s in word_tokenize(text)], dtype=torch.float32)\n",
    "    inputs = inputs.unsqueeze(0).to(device)\n",
    "    h = model.init_hidden(1)\n",
    "    tag, h = model(inputs, h)\n",
    "\n",
    "    print(\n",
    "        enc_tag.inverse_transform(\n",
    "            tag.argmax(-1).cpu().numpy().reshape(-1)\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
