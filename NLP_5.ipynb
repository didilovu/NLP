{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d6d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b550fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a080c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"sberquad\")\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "subsample = df_train.drop_duplicates('context').sample(100)\n",
    "paragraphs = subsample['context'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6a1e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors_from_texts(text,tokenizer,model):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    \n",
    "    if len(tokenized_text) > 512:\n",
    "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    tokens_tensor = tokens_tensor.to(device)\n",
    "    segments_tensors = segments_tensors.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2] # скрытые слои\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)#собираем скрытые слои\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)#получаем тензор без избыточных размерностей\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)# меняем порядое размерности, чтобы формат соответствовал\n",
    "    token_vecs_sum = []\n",
    "    \n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)#сумма последних 4-х токенов\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "        token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    return sentence_embedding.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81ced109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriv(question):#выбираем текст, в котором есть ответ на заданный вопрос\n",
    "    MODEL = 'DeepPavlov/rubert-base-cased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL, do_lower_case=True)\n",
    "    model = BertModel.from_pretrained(MODEL, output_hidden_states = True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    context_vectors = []\n",
    "    for paragraph in paragraphs:\n",
    "        context_vectors.append(vectors_from_texts(paragraph, tokenizer,model))\n",
    "    question_vector = vectors_from_texts(question, tokenizer,model)\n",
    "    vectors = []\n",
    "    \n",
    "    for vector in context_vectors:\n",
    "        vectors.append(cosine(vector, question_vector))\n",
    "\n",
    "    answ_par_id = vectors.index(min(vectors))\n",
    "    paragraph = paragraphs[answ_par_id]\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02ebbf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader(question, paragraph):#составляем точный ответ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"AlexKay/xlm-roberta-large-qa-multilingual-finedtuned-ru\")\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\"AlexKay/xlm-roberta-large-qa-multilingual-finedtuned-ru\", force_download=True, resume_download=False)\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(text=question,text_pair=paragraph)\n",
    "    \n",
    "    inputs = encoding['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs)\n",
    "    output = model(input_ids=torch.tensor([inputs]))\n",
    "    \n",
    "    start_index = torch.argmax(output[0])\n",
    "    end_index = torch.argmax(output[1])\n",
    "    answer = ' '.join(tokens[start_index:end_index+1])\n",
    "    \n",
    "    corrected_answer = ''\n",
    "\n",
    "    for word in answer.split(\" \"):\n",
    "        if word[:1] == \"▁\":\n",
    "            corrected_answer += ' ' + word[1:]\n",
    "        else:\n",
    "            corrected_answer += word\n",
    "    \n",
    "    return corrected_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51d81a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task5(question):\n",
    "    paragraph = retriv(question)\n",
    "    out_answ = reader(question, paragraph)\n",
    "    return out_answ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c655cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Согласно чему эмиссия совзнаков была прекращена?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af95a388a0246ec99d58b67584e9500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/781 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " декрету СНК СССР\n"
     ]
    }
   ],
   "source": [
    "question = subsample.sample()['question'].values[0]\n",
    "print(question)\n",
    "print(task5(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595d491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996cc083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
